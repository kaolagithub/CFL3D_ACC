NOTE: the most up-to-date information can be found on the web at:

http://cfl3d.larc.nasa.gov/Cfl3dv6/cfl3dv6.html

However, the information in this file is provided as a backup in case
connection to the web site cannot be made. However, it is generally
not as up to date or complete as the web site

Installation and use of CFL3DV6

1) cd cfl3dv6/build

2) make sure the file Install has execute permission!

3) type ./Install  [options]

   where [options] = 
             -single                (to create a makefile for single precision
                                     executables. Note: the default double
                                     precision makefile can be changed to
                                     create single precision executables
             -no_opt                (to create unoptimized executables with
                                     fast compilation but slow execution)
             -noredirect            (to dissallow redirected input file;
                                     needed for SP2)
             -purify                (to compile using Rational Software's 
                                     Purify memory checker
                                     - must have a licence for Purify
                                     - this option only effective on SGI 
                                       platform)
             -mpichdir=DIRm         (to use mpich on a workstation
                                     cluster; DIRm is the directory
                                     where mpich is located - not used
                                     on mpp machines like the SGI/Cray 
                                     Origin 2000, Cray T3E, IBM SP2, etc.)
             -cgnsdir=DIRc          (to use CGNS standard files in place of
                                     the native CFL3D grid and restart files;
                                     DIRc is the cgns base directory)
             -linux_compiler_flags=
               PG,Intel,Lahey,Alpha  (to chose the proper compiler FLAGS Linux
                                      machines; the compiler will be named with
                                      the generic name mpif77...you must either
                                      explicitly change the name in the 
                                      makefile that is generated via the 
                                      Install command or make sure that mpif77
                                      points to the correct compiler on your 
                                      system. NOTE: the default is Intel, i.e
                                      if you use the Intel ifc compiler on your
                                      Linux machine, you do not need to specify
                                      this option; if your default compiler is
                                      the Portland Group pgf90 compiler, then 
                                      you should use -linux_compiler_flags=PG)
             -help                    (to list the above options)
                                      
  if -no_opt is NOT specified, various compiler optimization levels are
  used to speed execution but results in slower compilation. It is 
  generally recommended that -no_opt NOT be specified.

  At this point you should have a makefile suited to the particular machine
  on which you are running. At the end of the install script, several
  messages are written out, indicating which platform it thinks you are
  using, where it assumes the mpi directories are, etc. the script will
  also suggest you type make to see a description of the various make
  options. you may also want to edit the makefile to see a layout of the
  directory structure of the cfl3dv6 system.

  MPP's will usually have MPI as part of the system. On workstation clusters,
  you will typically need to use MPICH, the portable, non-proprietary version
  of MPI. You may download MPICH free of charge from:

      http://www.mcs.anl.gov/mpi/mpich/index.html

  follow the installation directions that come with the MPICH release.

  To use the mpirun command with MPICH, you will also want to alias mpirun
  in your .cshrc, for example, on and SGI with 64 bit addressing:

  alias mpirun ~username/mpich/bin/mpirun 

  NOTE: for platforms that also have a native mpirun, you may want
  to alias mpichrun rather than mpirun 

4) if you type make at this point you will see a list of options, the
   most important options are:

   make cfl3d_seq      (make a sequential version of cfl3d)

   make cfl3d_mpi      (make a parallel (mpi) version of cfl3d

   make ronnie         (make a sequential version of the patched-grid
                        preprocessor, ronnie)

   make maggie         (make a sequential version of the overset-grid
                        preprocessor, maggie)

   make precfl3d       (make a sequential version of the array sizer)

   make splitter       (make a sequential version of the block splitter)

   make cfl3dcmplx_seq (make a sequential, complex version of cfl3d)

   make cfl3dcmplx_mpi (make a parallel, complex version of cfl3d)

   make splittercmplx  (make a sequential, complex version of splitter)



    NOTE: cfl3d version 6 has dynamic memory allocation, with precfl3d
          built in. Thus, it is no longer necessary to recompile the code
          for each case. This means that the user is not *required* to run
          precfl3d before executing and running cfl3d. However, the user
          may want to still run precfl3d before running cfl3d in order to
          see how much run-time memory (in WORDS) cfl3d will request. This
          is obviously useful to determine whether a particular problem 
          will fit in the memory of the machine, or to determine the 
          appropriate queue to submit the job to. Precfl3d will also 
          generate a file called ideal_speedup.dat that gives an UPPER
          bound of the  speedup that can be obtained by running particular
          problem on multiple processors as opposed to one.


   If you type make cfl3d_mpi, for example, the makefile will create and compile
   an mpi version to the code, and when complete, will indicate the location 
   of the executable RELATIVE TO THE CURRENT DIRECTORY (the build directory):

==============================================================
                                                              
                   DONE:  cfl3d_mpi created                     
                                                              
              the mpi executable can be found in:             
                                                              
                    cfl/mpi/cfl3d_mpi                            
                                                              
==============================================================

   All invocations of make "something-or-other" will produce similar messages
   when compilation has been succesfully completed


5) To run precfl3d with an input file called cfl3d.inp, type: 

          ./precfl3d -np num_nodes <cfl3d.inp

   Where: num_nodes = number of processors you wish to run cfl3d on

   NOTE: for parallel processing, one processor is (currently) required
   to serve as a host that essentially performs I/O duties. Thus, you 
   actually exploit only num_nodes-1 processors. 

   NOTE: Use num_nodes = 1 if only the sequential code is to be sized

   NOTE: num_nodes should never be larger than ngrids+1 (again, +1 for
   the host)

   To run cfl3d_mpi with an input file called cfl3d.inp, type:

         mpirun -np num_nodes ./cfl3d_mpi <cfl3d.inp

   (the same NOTES as above for the value of num_nodes apply)

   To run cfl3d_seq with an input file called cfl3d.inp, type:

         ./cfl3d_seq <cfl3d.inp


6) If patching or chimera options are used, compile ronnie or maggie
   as appropriate. Ronnie also has dynamic memory allocation, with
   preronnie built in. Thus, the stand-alone version of preronnie is
   now relegated to the role of providing a preview of how much memory
   ronnie will require at run time, but is no longer a prerequisite to
   executing ronnie. Only sequential versions of ronnie and maggie are
   available at this time.
   
   NOTE: currently, there is no preprocessor for maggie, and the code
   does not have dynamic memory. Thus, you must create a mag1.h
   file for your case, and copy that to the header directory
   (~username/cfl3dv6/header) before you can compile maggie for your case.
   More information on how to create a maggie input file can be found
   in cfl3dv6/source/maggie/maggie.doc

7) cd to your working directory and run cfl3d_mpi:

    mpirun -np num_nodes cfl3d_mpi <cfl3d.inp & (omit the <cfl3d.inp if
                                                 running on the SP2 and
               ^^^^^^^^^                         -noredirect was specified
                                                  at installation)

    NOTE that num_nodes must account for a host process. Thus, if you
    have a grid with 4 equal-sized blocks, and you want to run each block
    on a separate processor, you must set num_nodes = 5 (=4+1)

    to run the sequential build of the code, you do not need mpirun,
    and can simply type:

    cfl3d_seq <cfl3d.inp &

8) On workstation clusters, once you have installed MPICH (see the
   documentation that comes with MPICH for details), you may use mpirun 
   as above, or more typically, create a file called (for example)
   pg_file, where pg_file might contain, for a case to be run on a total
   of 7 procssors:

     local 0
     aamb6-f 1 /net/cfd14/scratch/biedron/hsct/cfl3d_mpi
     aamb6-f 1 /net/cfd14/scratch/biedron/hsct/cfl3d_mpi
     aamb7-f 1 /net/cfd14/scratch/biedron/hsct/cfl3d_mpi
     aamb7-f 1 /net/cfd14/scratch/biedron/hsct/cfl3d_mpi
     aambsgi-f 1 /net/cfd14/scratch/biedron/hsct/cfl3d_mpi
     aambsgi-f 1 /net/cfd14/scratch/biedron/hsct/cfl3d_mpi

   in the above pgfile example, "aamb6-f", "aamb7-f" and "aambsgi-f" are each
   dual-processor SGI Octanes. These are the "compute" nodes.
   The host process will execute on "local 0",the processor on which the mpirun
   command below is issued. The mpi executable is called cfl3d_mpi, and
   is located in the directory /net/cfd14/scratch/biedron/hsct in this example.

   The parallel code is executed by typing:

   mpichrun -p4pg pg_pgfile cfl3d_mpi <cfl3d.inp

   It is implied in the example above that mpich has been installed with the
   -device=ch_p4 option (see the mpich installation manual), and that the
   mpirun command in the mpich/bin directory has been aliased to mpichrun.


---------------------------------------------------------------------
IMPORTANT NOTES FOR SGI WORKSTATIONS:

1) for IRIX64 architectures (R10000 chips) using mpich, the 
   fortran options that are built into the makefile assume that 64 bit 
   addressing is used. So to be consistent (and to get the object
   modules and mpi libraries to link), mpich must be installed
   in a consistent manner. Specifically, the following mpich
   configure options seem to work:

    configure -arch=IRIX64 -cc="cc -64 -mips4 -r10000" \
                           -fc="f77 -64 -mips4 -r10000" \
                           -opt="-O2" \
                           -device=ch_p4

IMPORTANT NOTES FOR MPICH ON HEWLET PACKARD WORKSTATIONS:

1) Currently, there seems to be no way to use double precision AND mpich 
on the hp...single precision seems to work with the following mod:

2) According to Samson Cheung, the following mod should be made:

in ~mpich/lib/hpux/ch_p4/mpif77, 

change:

    $Show f77 $finc  +T $compileargs

to
    $Show f77 $finc  $compileargs

this change should be made *before* cfl3d is compiled; otherwise recompile
cfl3d after the change (type make scruball followed by make linkall before
recompiling to insure a clean recompilation)

IMPORTANT NOTES FOR IBM WORKSTATIONS:

1) There appears to be an odd "feature" of the PBS (Portable Batch Script) 
system on the NASA SP2's at Ames and Langley. If the input file (typically
called cfl3d.inp) is larger than 32 Kbyte, and the standard way of running
cfl3d is used, with redirected input (e.g. mpirun -np=8 ./cfl3d_mpi <cfl3d.inp)
then the process will hang, and use up cpu time without doing anything at all.
For many multiblock cases the input file can easily be > 32 Kb. The solution 
is to not use redirected input. An installation option is available that 
does not use redirected input: ./Install -noredirect   If this option is
used (recommended on SP2's, at least those at Ames/Langley), then your input
file MUST be called cfl3d.inp, and do NOT use redirected input when running
cfl3d. That is with the -noredirect installation option, the standard mpirun
procedure should be, e.g.,  mpirun -np=8 ./cfl3d_mpi  If running on an rs6000
(the basic SP2 processor), the installation script will issue a message 
suggesting you use the -noredirect option if you did not do so, or, if you 
did, reminding you to name your input file cfl3d.inp and to not use file
redirection for the input deck.

2) Compilation (at least on the NASA SP2's) is very slow if the full 
optimization is used. For initial testing, it may be desirable to use
-debug installation option (./Install -debug) to generate a slower executing,
but much faster compiling code. 


